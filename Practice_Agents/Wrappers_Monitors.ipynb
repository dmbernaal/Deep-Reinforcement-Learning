{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrappers & Monitors\n",
    "The ```Wrapper``` class inherits the ```Env``` class. The only argument the wrappers constructor accepts is: The instance of the ```Env``` class to be wrapped. \n",
    "\n",
    "These are some subclasses of ```wrapper``` that allow for filtering of only a specific portion of information:\n",
    "\n",
    "**```ObservationWrapper```**: You will need to redefine ```observation(obs)``` method of the parent. The ```obs``` argument is an observation from the wrapped environment, and this method should return the observation that will be given to the agent. \n",
    "\n",
    "**```RewardWrapper```**: This will expose the ```reward(rew)``` method, which could modify the reward value given to the agent\n",
    "\n",
    "**```ActionWrapper```**: You will need to override the ```action(act)``` method, which could tweak the action passed to the wrapped environment to the agent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomActionWrapper(gym.ActionWrapper):\n",
    "    def __init__(self, env, epsilon=0.1):\n",
    "        \"\"\"\n",
    "        epsilon: A probability of random action. In our case, we will choose a random action 10% of the time\n",
    "        \"\"\"\n",
    "        super(RandomActionWrapper, self).__init__(env)\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "    def action(self, action):\n",
    "        \"\"\"\n",
    "        Every turn we will 'roll a die' and with the probability of epsilon, we sample a random action from the action space and return it instead of the action the agent has sent to us.\n",
    "        \"\"\"\n",
    "        if random.random() < self.epsilon:\n",
    "            print('Random Action')\n",
    "            return self.env.action_space.sample()\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Action\n",
      "Random Action\n",
      "\n",
      "Steps achieved: 10\n",
      "Total Reward: 10.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    env = RandomActionWrapper(gym.make('CartPole-v0'))\n",
    "    \n",
    "    obs = env.reset()\n",
    "    total_reward = 0.0\n",
    "    total_steps = 0\n",
    "    \n",
    "    while True:\n",
    "        obs, reward, done, _ = env.step(0) # push right\n",
    "        total_reward += reward\n",
    "        total_steps += 1\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "    print(f'\\nSteps achieved: {total_steps}')\n",
    "    print(f'Total Reward: {total_reward}\\n')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitor\n",
    "This class is also implemented like a ```Wrapper``` and can write information about your agent's performance in a file with an optional video recording of your agent in action "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodes done: 13\n",
      "Total Reward: 13.0\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    env = gym.make('CartPole-v0')\n",
    "    env = gym.wrappers.Monitor(env, 'recording', force=True)\n",
    "    \n",
    "    total_reward = 0.0\n",
    "    total_steps = 0\n",
    "    obs = env.reset()\n",
    "    \n",
    "    while True:\n",
    "        action = env.action_space.sample()\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        total_steps += 1\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "    print(f'Episodes done: {total_steps}\\nTotal Reward: {total_reward}')\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

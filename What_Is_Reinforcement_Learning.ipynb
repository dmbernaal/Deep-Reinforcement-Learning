{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Reinforcement Learning?\n",
    "Reinforcement Learning is a subfield of *machine learning* which addressed the problem of automatic learning of optimal decisions over time. \n",
    "\n",
    "**Reinforcment Learning** is an approach that natively incorporates the extra dimension (hidden time dimension, causing dynamic change) into learning equations, which puts it much closer to *Human Perception* of *Artificial Intelligence* \n",
    "\n",
    "**RL** lays somewhere in between full supervision and a complete lack of predefined labels. It uses many well-established methods of supervised learning such as deep neural networks for function approximation, stochastic gradient descent, and backpropogation, to learn data represenetations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Makes Reinforcement Learning Tricky?\n",
    "\n",
    "#### One\n",
    "The **First** thing to note is that **observation** in RL depends on an agent's behavior and to some extent, it is the **result** of their behavior. \n",
    "\n",
    "If your agent decides to do inefficient things, then the observations will tell you nothing about what they have done wrong and what should be done to improve the outcome (What is we applied mentors - a form of communicative transfer learning). \n",
    "\n",
    "If the agent is stubborn and keeps making mistakes, then the observatuons can make the false impression that there is no way to get a larger reward which could be totally wrong. \n",
    "\n",
    "#### Two\n",
    "The **Second** thing that complicates our agent's life is that they need to not only **exploit** the policy they have learned, but to **actively explore** the enviroment (It must be curious), because, who knows, maybe by doing things differently we can significantly improve the outcome we get. \n",
    "\n",
    "The problem is that too much exploration may also seriously decrease the reward or the agent may forget what was optimal. This balance between: Exploration/Exploitation is one of the open fundamental questions in RL. \n",
    "\n",
    "*In a real life setting: \"Should I go to an already known place for dinner or try this new fancy restaurant? How frequently should you change jobs? Should you study a new field or keep working in your area?\"*\n",
    "\n",
    "#### Three\n",
    "The **Third** complication factor lays in the fact that reward can be seriously delayed from actions. Example: In cases of chess, it can be one single strong move in the middle of the game that has shifted the balance. Durling learning, we need to discover such casualties, which can be tricky to do over the flow of time and our actions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reward\n",
    "In RL, **Reward** is just a scalar value we obtain periodically from the enviroment. It can be positive or negative, large or small, but it's just a number. \n",
    "\n",
    "The purpose of reward is to tell our agent how well they have behaved (like an objective function). It's common practice to recieve a reward every fixed timestamp or every enviroment interaction. \n",
    "\n",
    "The term *Reinforcement* comes from the fact that a reward obtained by an agent should *reinforce* its behavior in a positive or negative way. Reward is *local*, meaning, it reflects the success of the agent so far. \n",
    "\n",
    "What the agent is trying to **achieve** is the largest *Accumulated* reward over its sequence of actions. Examples:\n",
    "\n",
    "* **Financial Trading**: An amount of profit is a reward for a trader buying and selling stocks \n",
    "\n",
    "* **Chess**: Here, reward is obtained at the end of the game, as win, loss, or draw\n",
    "\n",
    "* **Dopamine systen in a brain**: There is a part in the brain that produces dopamine every time it needs to send a positive signal to the rest of the brain. Higher concentrations of dopamine lead to a sense of pleasure, which *reinforce* activities considered by this system as *good*.\n",
    "\n",
    "* **Computer Games**: They usually get obvious feedback to the player, which is either the number of enemies killed or a score gathered. The RL reward for aracade games should be the derivative of the score, that is, +1 every time a new enemy is killed and 0 at all other time steps. \n",
    "\n",
    "* **Web Navigation**: There is a set of problems with high practical value, which is to be able to automatically extract information present on the web. The RL-based approach to these tasks, in which the reward is the information or the outcome you need to get \n",
    "\n",
    "* **Neural Network Architecture Search**: RL has been successfully applied to the domain of NN Architecture Optimization, where the aim is to get the best performance metric on some dataset by tweaking the number of layers or their parameters, adding extra bypass connection, or making other changes to the neural network architecture. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Agent\n",
    "An agent is somebody or something who/which interacts with the enviroment by executing certain actions, taking observations, and recieving eventual rewards for this. Examples:\n",
    "\n",
    "* **Financial Trading**: A Trading system or a strader making decisions about order execution \n",
    "\n",
    "* **Chess**: A player or a computer program\n",
    "\n",
    "* **Dopamine System**: The brain itself, according to sensory data, decides if it was a good or bad experience \n",
    "\n",
    "* **Computer Games**: The player who enjoys the game or the computer program\n",
    "\n",
    "* **Web Naviagation**: The software that tells the browser which links to click on, where to move the mouse, or which text to enter\n",
    "\n",
    "* **Neural Network Architecture Search**: The software that controsl the concrete architecture of the neural network being evaluated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Environment\n",
    "The enviroment is everything outside of an agent. In the most general sense, it's the rest of the **universe**. \n",
    "\n",
    "The enviroment is external to an agent, and its communication with the enviroment is limited by rewards, actions, and observations\n",
    "\n",
    "Rewards: Obtained from the enviroment. Actions: Executed by the agent and given to the enviroment. Observations: Some information besides the rewards that the agent recieves from the enviroment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actions\n",
    "Actions are things that an agent can do in the environment. In RL, we distinguid between two types of actions: **discrete** or **continous**. \n",
    "\n",
    "**Discrete** actions form the finite set of mutually exclussive things an agent could do, such as: Move left, Move right. \n",
    "\n",
    "**Continous** actions have some value attached to the action, such as a car's action: *Steer the wheel* having an angle and direction of steering. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "Observations of the enviroment is the second information channel for an agent, with the first being a reward. Observations are pieces of infromation that the enviroment provides the agent with, which say what's going on around them. \n",
    "\n",
    "It's also important to distinguid between an enviroment's state and observations. The state of an enviroment potentially includes every atom in the unverse, which makes it impossible to measure everything about the environment. \n",
    "\n",
    "Examples:\n",
    "* **Financial Trading**: Here the enviroment is the whole financial market and everything that influences it. This is a hude list of things such as: Latest news, economic and political conditions, weather, food supplies, and twitter trends. \n",
    "\n",
    "* **Chess**: The enviroment here is your board plus your opponent which inlcudes their chess skills, mood, brain state, chosen tactics, and so on. \n",
    "\n",
    "* **Dopamine System**: The enviroment here is your brain PLUS nervous system and organ's states PLUS the whole world you can percieve. \n",
    "\n",
    "* **Computer Game**: Here, the enviroment is your computer's state, including all memory and disk data. Observations are a screen's pixels and sound. \n",
    "\n",
    "* **Web Navigation**: The environment here is the internet, including all the network infrastructure between the computer our agent works and the web server. Observation is normally the web page that is loaded at the current navigation step \n",
    "\n",
    "* **Neural Network Architecture Search**: The enviroment is fairly simple and includes the NN toolkit that performs the particular neural network evaluation and the dataset taht is used to obtain the performance metric. Observations might be different and inlcude some information about the testing, such as loss convergence dynamics or other metrics obtained from the evaluation step. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

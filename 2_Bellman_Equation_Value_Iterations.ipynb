{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Bellman Equation of Optimality\n",
    "\n",
    "### Deterministic Case\n",
    "Watch this: https://www.youtube.com/watch?v=14BfO5lMiuk\n",
    "\n",
    "Suppose the following *Deterministic* case: Where all our actions have a 100% guarantee outcome. Imagine that out agent observes state $s_0$ and has $N$ available actions to take. \n",
    "\n",
    "Every action leads to another state, $s_1...S_N$, with a respective reward, $r_1...r_N$. \n",
    "\n",
    "We will also assume that we know the values, $V_i$, of all states connected to the initial state $s_0$. \n",
    "\n",
    "*What will be the best course of action that the agent can take in such a state?* \n",
    "\n",
    "Say we now choose the concrete action $a_1$, and calculate the value given this action, then the value will be: $V_0(a=a_i) = r_i + V_i$. \n",
    "\n",
    "For the agent to choose the right action, the agent will need to calculate the resulting values for every action and choose the maximum possible outcome: \n",
    "\n",
    "$$V_0 = max_a\\sum{1...N}(r_a + V_a)$$\n",
    "\n",
    "If we are using a discount factor $\\gamma$ we will use:\n",
    "\n",
    "$$V_0 = max_a\\sum{1...N}(r_a + \\gamma V_a)$$\n",
    "\n",
    "### Stochastic Case\n",
    "Watch this: https://www.youtube.com/watch?v=aNuOLwojyfg \n",
    "\n",
    "\n",
    "What we will need to do in a stochastic case is calculate the expected value for every action, instead of just taing the value of the next state. \n",
    "\n",
    "So let's consider the following: \n",
    "\n",
    "Let's consider one single action available from state $s_0$, with three possible outcomes. That is a single action can lead to different states with different probabilities: \n",
    "\n",
    "$p_1 = s_1$, $p_2 = s_2$ , $p_3 = s_3$. All probabilities must sum up to one: $p_1 + p_2 + p_3 = 1$. \n",
    "\n",
    "Every target state has it's own reward $s_1 = r_1$, $s_2 = r_2$, $s_3 = r_3$. \n",
    "\n",
    "Here is how we calculate the expected value after issuing a single action (1): \n",
    "\n",
    "$$V_0(a=1) = p_1(r_1 + \\gamma V_1) + p_2(r_2 + \\gamma V_2) + p_3(r_3 + \\gamma V_3)$$\n",
    "\n",
    "Which can be expressed as the sum of all values, multiplied by their probabilities. \n",
    "\n",
    "By combining the Bellman Equation, for a deterministic case, with a value for stochastic actions, we get the **bellman optimality equation**: \n",
    "\n",
    "$$V_0=\\max _{a \\in A}\\sum_{s \\in S}P_{a,0 \\rightarrow s}(r_{s,a} + \\gamma V_s)$$\n",
    "\n",
    "So now (as before): The optimal value of the state is equal to the action, which gives us the maximum possible expected immediate reward, plus discounted long-term reward for the next state. \n",
    "\n",
    "The definition is **recursive** meaning: The value of the state is defined via the values of immediate reachable states. \n",
    "\n",
    "These values not only give us the best reward that we can obtain, but they basically give us the optimal policy to obtain that reward. \n",
    "\n",
    "If our agent knows the value for every state, then it automatically knows how to gather all this reward. Therefor at every state the agent ends up in, it needs to select the action with the maximum expected reward for the action: A sum of the immediate reward and the one-step discounted long-term reward. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value of action\n",
    "To make our life sligthly easier, we can define different quantities in addition to the value of state $V_s$: **Value of action: $Q_{s,a}$**. \n",
    "\n",
    "This equals the total reward we can get by executing an action $a$ in state $s$ and can be defined $V_s$. \n",
    "\n",
    "This quantity gave a name to a whole family of methods: **Q-Learning** Learn more here: https://www.youtube.com/watch?v=aCEvtRtNO-M \n",
    "\n",
    "In these \"Q-Learning\" methods, our primary objective is to get values $Q$ for every pair of state and action. \n",
    "\n",
    "$$Q_{s,a}=\\sum_{s\\prime \\in S}P_{a,s \\rightarrow s\\prime}(r_{s,a} + \\gamma V_s\\prime)$$\n",
    "\n",
    "Or in other words: $Q$ for this state $s$ and action $a$ equals ***the expected immediate reward and the discounted long-term reward of the destination state.***\n",
    "\n",
    "We can also define this as: \n",
    "\n",
    "$$V_s = max_{a \\in A}Q_{s,a}$$\n",
    "\n",
    "Meaning: The value of some state equals to the value of the maximum action we can execute from this state.\n",
    "\n",
    "$Q$ values are much more convenient in practice, as for the agent its much simpler to make decisions about actions baed on $Q$ than based on $V$. \n",
    "\n",
    "In the case of $Q$, for the agent to choose the action based on the state, the agent just need to calculate $Q$ for all available actions, using the current state and choose the action with the largest value of $Q$. \n",
    "\n",
    "If the agent were to do the same using values of states, the agent needs to know not only values, but also probabilities for transitions. In practice, we rarely know them in advance, so the agent needs to estimate transition probabilities for every action and state pair. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Value Iteration Method\n",
    "Watch: https://www.youtube.com/watch?v=4KGC_3GWuPY\n",
    "\n",
    "The **Value iteration algorithm** allows us to numerically calculate the values of states and values of actions of MDPs with known transition probabilities and reward. \n",
    "\n",
    "The procedure (for values of states) includes the following steps: \n",
    "1. Initialize values of all states $V_i$ to some initial value (usually zero)\n",
    "2. For every state $s$ in the MDP, perform the **Bellman Update**\n",
    "3. Repeat step 2 for some large number of steps or until changes become too small\n",
    "\n",
    "In the case of **Action Values (Q)**, only monitor modifications to the preceding procedure are required: \n",
    "1. Initialize all $Q_{s,a}$ to zero\n",
    "2. For every state $s$ and every action $a$ in this state, perform update: $Q_{s,a} \\leftarrow \\sum_{s\\prime} P_{a,s \\rightarrow s \\prime}(r_{s,a} + \\gamma max_a\\prime Q_{s\\prime,s\\prime})$\n",
    "3. Repeat step 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Process\n",
    "The **Markov Process** is also known as a **Markov Chain**. \n",
    "\n",
    "*Imagine that you have some system in front of you that you can only observe. What you observe is called **states**, and the system can switch between states according to some laws of dynamics. Though you cannot influence the system, only observe.* \n",
    "\n",
    "**State Space** are all possible states for a system. Which in Markov Processes, we require this set of states to be finite. Your observations form a sequence of states hence **Chain**. \n",
    "\n",
    "**History** is a sequence of observations over time forming a chain of states: [*sunny, sunny, rainy, sunny,.... cloudy*]\n",
    "\n",
    "Though in order to call a *system* a Markov Process it needs to fulfill the **Markov Property**: Meaning that the future system dynamics from any state have to depend on this state only. Therefor the Markov Property requires the states of the system to be distinguihable from each other and unique. Therefor, only one state is requires to model the future dynamics of the system, not the whole history or, say, the last N states. \n",
    "\n",
    "As your system model complies with the Markov Property, you can capture transition probabilities with a **Transition Matrix**: A square matrix of size $NxN$, where $N$ is the number of states in our model. Every cell in a row $i$ and a column $j$ in the matrix contains the probability of the system to transition from state $i$ to state $j$\n",
    "\n",
    "So the formal definition of Markov Processes are:\n",
    "* A set of states (S) that a system can be in\n",
    "* A transition matrix (T), with transition probabilities, which defines the systems dynamics\n",
    "\n",
    "**In practice**, we rarely have the luxury of knowing the exact transition matrix. A much more real-world situation is when we have only observations of our system's states also known as **Episodes**. \n",
    "\n",
    "Though: *It's not complicated to estimate the transition matrix by our observations. We just count all the transitions from every state and normalize them to a sum of 1. The more observation data we have, the close our estimation will be to the true underlying model.* \n",
    "\n",
    "Another important note is that Markov Property implies **Stationarity** which is the: the underlying transition distribution for any state does not change over time. \n",
    "\n",
    "**Non-Stationarity** means that there are some hidden factors that influence our systems dynamics, and this factor is not included in observations. \n",
    "\n",
    "We must be able to know with probabilty (suming up to 1) that something will occur. If this 'episode' is not accounted for and therefor hidden, then there is no Markov Property. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov Reward Process\n",
    "We now will extend our Markoc Process Model. \n",
    "\n",
    "First, we need to add value to our tranisiton from state to state. \n",
    "\n",
    "Reward can be represented in various forms. The most general way is to have another *Square Matrix* similar to the tranisition matrix with rewards for transitioning from state $i$ to state $j$ residing in row $i$ and column $j$. \n",
    "\n",
    "They can be positive or negative, large or small. \n",
    "\n",
    "The Second thing we're adding to the model is a **discount factor** $\\gamma$: A single number from 0 to 1. \n",
    "\n",
    "Now, all our observations have a reward value attached to every tranisiton summary. \n",
    "\n",
    "For every episode, we define **Return** at time $t$ as:\n",
    "\n",
    "$$G_t = R_{t+1}+\\gamma R_{t+2}+... = \\sum_{k=0}^{\\infty}\\gamma^k R_{t+k+1}$$\n",
    "\n",
    "* $G_t$ : Our return\n",
    "* $R_{t+i}$ : Our reward at timestep $i$\n",
    "* $\\gamma$ : Our discount factor (between 0 to 1)\n",
    "* $k$ : Number of steps we are away from starting point $t$\n",
    "* $t$ : Our initial starting point\n",
    "\n",
    "Therefor, for every time point, we calculate **return** as: *Sum of subsequent rewards, but more distant rewards are multiplied by the discount factor raised to the power of the number of steps we are away from the starting point at $t$.* \n",
    "\n",
    "* If $\\gamma = 1$, then $G_t$ equals a sum of all subsequent rewards and corresponds to the agent with **perfect visibility of any subsequent rewards**\n",
    "* If $\\gamma = 0$, our return $G_t$ will be just immediate reward without any subsequent state and correspond to absolute **short-sightedness**\n",
    "\n",
    "Therefor, think about $\\gamma$ as a measure of how far into the future we look to estimate the future return: The closer to 1, the more steps ahead of us we take into account\n",
    "\n",
    "\n",
    "If we go to the extremes and calculate the mathematical expectation of return for any state (averaging large amount of chains), we'll get: **Value of State:**\n",
    "$$V(s) = \\mathbb{E}[G|S_t=s]$$\n",
    "\n",
    "For every state $s$, the value $v(s)$ is the average (or expected) return we get by following the Markov Reward Process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov Decision Process\n",
    "We will extend MRP to include actions into the picture. First, we must add a set of actions $A$, which has to be finite. This will be our agents *Action Space*\n",
    "\n",
    "Then, we need to condition our transition matrix with action, which basically means our matrix needs an extra *action* dimension, which turns into a cube. \n",
    "\n",
    "Now the agent no longer passively observes state transitions, but can actively choose an action to take at every time. We will now have a Matrix at every state. \n",
    "\n",
    "*Depth Dimension* contains actions that the agent can take, and the other dimensions is that the target state system will jump to after this action is performed by the agent. \n",
    "\n",
    "The general **MDP** has a *3D transition matrix* with dimensions: (Source State, Action, Target State).\n",
    "\n",
    "Now our reward matrix will depend not only on state but also on action. Therefor the reward the agent obtains now depends not only on the state it ends up in but also on the action that leads to this state. \n",
    "\n",
    "#### Policy\n",
    "The most important central thing for MDPs and RL is **Policy**. The *intuitive* definition: Policy is some set of rules that control the agent's behavior. Even for simple environemnts, we can have a variety of policies. \n",
    "\n",
    "The main objective of the agent in RL is to gather as much return as possible. So intuitively, different policies can give us different return, which makes it important to find a good policy. \n",
    "\n",
    "Formal definition: The probability distribution over actions or every possible state:\n",
    "$$\\pi(a|s)=P[A_t=a|s_t=s]$$\n",
    "\n",
    "We introduce this as *probability*, not as a concrete action, to introduce randomness into an agents behavior. \n",
    "\n",
    "*If our policy is **Fixed** and not changing, then our MDP becomes an MRP, as we can reduce transition and reward matrices with a policy's probabilities and get rid of action dimensions* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

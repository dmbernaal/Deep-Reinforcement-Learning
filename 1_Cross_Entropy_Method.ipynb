{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Entropy Method\n",
    "Some strengths of this method are:\n",
    "* **Simplicity**: This is a very simple method, which makes it an intuitive method to follow. \n",
    "* **Good convergence**: In simple environments that don't require complex, multistep policies to be learned and discovered and have short episodes with frequent rewards, cross-entropy usually works well. \n",
    "\n",
    "## RL Methods:\n",
    "* Model-free or Model-based\n",
    "* Value-based or Policy-based\n",
    "* On-policy or Off-policy\n",
    "\n",
    "**Cross Entropy Method** falls into *Model-free** and *Policy-based*. \n",
    "\n",
    "**Model-Free** means that the method doesn't build a model of the environment or reward; it just directly connects observations to actions (or values that are related to actions). Therefor the agent takes current observations and does some computations on them, and the result is the action that it should take. \n",
    "\n",
    "*Model-Free* are usually esier to train as its hard to build good models of complex enviroment with rich observations. \n",
    "\n",
    "*This is more how humans operate, we don't everything about the world of environment and base our judgement/decisions on our observation + (stored knowledge of the world/enviroment)* \n",
    "\n",
    "**Model-Based** methods try to predict what the next observation and/or reward will be. Based on this prediction, the agent is trying to choose the best possible action to take, very often making such precictions multiple times to look more nad more steps into the future (Monte-Carlo-Tree-Search). \n",
    "\n",
    "*Model-Based* are used in deterministic environments, such as: board games with strict rules. \n",
    "\n",
    "**Policy-Based** methods are directly approximating the policy of the agent: What actions the agent should carry out at every step. \n",
    "\n",
    "*Policy* is usually represented by probability distribution over the available actions\n",
    "\n",
    "**Value-Based**: Instead of probability of actions, the agent calculates the value of every possible action and chooses the action with the best value. \n",
    "\n",
    "**Off-Policy**: The ability of the method to learn on old historical data (obtained by a previous version of the agent or recorded by human demonstration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Entropy Method\n",
    "This method is: **Model-Free, Policy-Based, On-Policy**\n",
    "* It doesn't build any model of the environment.\n",
    "* It approximates the policy of the agent\n",
    "* It requires fresh data obtained from the environment\n",
    "\n",
    "## Practical Cross-Entropy\n",
    "We follow a common ML approach, replacing all of the complications of the agent with some kind of nonlinear trainable function, which maps the agent's input (Observation from the environment) to some output. \n",
    "\n",
    "For our cross-entropy method: our nonlinear function is a (Deep Neural Network) producing our *policy*, which basically says for every observation which action the agent should take. \n",
    "\n",
    "Observation --> NN --> Policy\n",
    "\n",
    "In practice, policy is usually represented as probability distribution over actions, which makes it very similar to a classification problem, with the amount of classes being equal to amount of actions we can carry out. \n",
    "\n",
    "We need to pass an observation from the environment to the neural network, get probability distribution over actions, and perform random sampling using probability distribution to get an action to carry out. \n",
    "\n",
    "Loop: \n",
    "At the beginning of the training when our weights (NN) are random, the agent behaves randomly. After the agent gets an action to issue, it fires the action to the environment and obtains the next observation and reward for the last action. \n",
    "\n",
    "During the agent's lifetime, its experience is presented as episodes. Every episode is a sequence of observations that the agent has got from the environment, actions it has issues, and rewards for these actiosn. \n",
    "\n",
    "*After our agent has played several such episodes. For every episode, we can calculate the total reward that the agent has claimed. For simplicity this will just be a sum of all local rewards for every episode. This total reward will show how good this episode was for the agent.* \n",
    "\n",
    "Example: \n",
    "```episode_i``` = $R = r_j + r_{j+1} + ... + r_n$\n",
    "* ```i``` = episode number\n",
    "* $R$ = Total reward for that episode\n",
    "* $r_j$ = reward for observation at timestep j\n",
    "* $r_n$ = the last reward for the episode\n",
    "\n",
    "Each episode is composed of cells (timesteps) with: ```(observation, action, reward)```\n",
    "\n",
    "Every cell represents the agents step in the episode. \n",
    "\n",
    "The **core** of cross-entropy method is to throw away bad episodes and train on better ones. \n",
    "\n",
    "1. Play $N$ number of episodes using our current model and environment\n",
    "2. Calculate the total reward for every episode and decide on a reward boundary. *Usually, we use some percentile of all rewards: 50th or 70th\n",
    "3. Throw away all episodes with a reward below that boundary\n",
    "4. Train on something remaining **elite** episodes using observations as the input and issued actions as the desired output\n",
    "5. Repeat from step 1 until we become satisfied with the result "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limitations of Cross-Entropy Method\n",
    "* For training, our episodes have to be finite and, preferably, short\n",
    "* The total reward for the episodes should have enough variability to seperate good episode from bad ones. \n",
    "* There is no intermediate indication about whether the agent has succeeded or failed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FrozenLake\n",
    "Brief into on FrozenLake: It's a grid-world. Your agent lives in a grid of size 4x4 and can move in four directions: Up, Down, Left, and Right. \n",
    "\n",
    "The agent always starts at a top-left position, and its goal is to reach the bottom-right cell of the grid. \n",
    "\n",
    "There are holes in the fixed cells of the grid and if you get into those holes, the episode ends and your reward = 0. If the agent reaches the destination cell, then it obtaiend the reward 1.0 and episode ends. \n",
    "\n",
    "The world is also slippery hence (Frozen lake), so the agent's actions do not always turn out as expected: **33%** chance that it will slip to the right or to the left. \n",
    "\n",
    "*Below we will have code with details. That is it will mirror the python script and will be explained in detail so you know what is going on and how to construct!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many spaces are there? \n",
      "Discrete(16)\n",
      "\n",
      "How many actions can we take? \n",
      "Discrete(4)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "e = gym.make('FrozenLake-v0')\n",
    "\n",
    "print(f'How many spaces are there? \\n{e.observation_space}\\n')\n",
    "print(f'How many actions can we take? \\n{e.action_space}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discrete** means we can take an absolute position or that it's an abosolute position of space. Meaning it's not continous such as: Climbing a hill, steering a wheel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "# intializing\n",
    "e.reset()\n",
    "e.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So that constructed our observation_space grid (4x4) which is:\n",
    "* **S**: in red is our start. \n",
    "* **G**: at the bottom right is our goal.\n",
    "* **H**: is a hole (you will fall!)\n",
    "* **F**: frozen step (this is slippery, like super slippery)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now implement **One-Hot-Encoding** to our matrix and action steps, because our NN can only take Tensors which will be floating point numbers. \n",
    "\n",
    "This will be wrapped using: ```ObservationWrapper``` class from Gym."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-Hot Function\n",
    "class DiscreteOneHotWrapper(gym.ObservationWrapper):\n",
    "    \"\"\"\n",
    "    This should make our game playable with our other Agent (CartPole). But this isn't enough for the agent to learn.\n",
    "    \"\"\"\n",
    "    def __init__(self, env):\n",
    "        super(DiscreteOneHotWrapper, self).__init__(env)\n",
    "        assert isinstance(env.observation_space, gym.spaces.Discrete)\n",
    "        \n",
    "        self.observation_space = gym.spaces.Box(0.0, 1.0, (env.observation_space.n, ), dtype=np.float32)\n",
    "        \n",
    "    def observation(self, observation):\n",
    "        res = np.copy(self.observation_space.low)\n",
    "        res[observation] = 1.0\n",
    "        return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweaking Cross-Entropy Method\n",
    "Because of the limitations, we will need to tweak our Cross-Entropy Agent a bit for it to work on this environment. \n",
    "\n",
    "Such as: \n",
    "* **Larger batches of played episodes**: FrozenLake requires at least 100 (episodes) to get some successful episodes \n",
    "* **Discount factor applied to reward**: To make the total reward for the episode depend on episode lenght, and add variety in episodes, we can use a discounted total reward with the discounted factor: 0.9 or 0.95. Therefor, the reward for shorter episodes will be higher than the reward for longer ones. \n",
    "* **Keeping 'elite' episodes for a longer time**: Instead of throwing them away after training on the best ones. In FrozenLake, a successful episode is a much rarer animal: so we need to keep them for severl iterations to train on them.\n",
    "* **Decrease learning rate**: This will give our network time to average more training samples\n",
    "* **Much longer training time**: To reach 50% successful episodes, about 5k training iterations are required. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changes to filter_batch() function\n",
    "def filter_batch(batch, percentile):\n",
    "    \"\"\"\n",
    "    We will now calculate discounted reward & return elite episodes (keep track of them)\n",
    "    \"\"\"\n",
    "    disc_rewards = list(map(lambda s: s.reward * (GAMMA ** len(s.steps)), batch))\n",
    "    \n",
    "    reward_bound = np.percentile(disc_rewards, percentile)\n",
    "    \n",
    "    train_obs = []\n",
    "    train_act = []\n",
    "    elite_brach = [] # keeping track of winners\n",
    "    \n",
    "    for example, discounted_reward in zip(batch, disc_rewards):\n",
    "        if discounted_reward > reward_bound:\n",
    "            \n",
    "            train_obs.extend(map(lambda step: step.observation, example.steps))\n",
    "            train_act.extend(map(lambda step: step.action, example.steps))\n",
    "            \n",
    "            elite_batch.append(example)\n",
    "    \n",
    "    return elite_batch, train_obs, train_act, reward_bound "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding this to training loop\n",
    "\"\"\"\n",
    "In the training loop, we will store previous 'elite' episodes to pass them to the preceding function on the next training iteration\n",
    "\"\"\"\n",
    "    # Training loop\n",
    "    full_batch = []\n",
    "    for iter_no, batch in enumerate(iterate_batches(env, net, BATCH_SIZE)):\n",
    "        reward_mean = float(np.mean(list(map(lambda s: s.reward, batch))))\n",
    "\n",
    "        full_batch, obs, acts, reward_bound = filter_batch(full_batch + batch, PERCENTILE)\n",
    "\n",
    "        if not full_batch:\n",
    "            continue\n",
    "\n",
    "        obs_v = torch.Tensor(obs)\n",
    "        acts_v = torch.LongTensor(acts)\n",
    "        full_batch = full_batch[-500:] # get last 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

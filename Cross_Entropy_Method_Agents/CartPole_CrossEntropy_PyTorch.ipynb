{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CartPole Pytorch\n",
    "In this notebook we will discuss in detail the methods and code that go into ```cartpole_1.py```\n",
    "\n",
    "In this specific notebook and corresponding script we will be using the raw **PyTorch** framework whereas in others we use **FastAI** framework along with various Convolutional Neural Archictures for fine-tunned models \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our hidden layer will contain 128 neurons\n",
    "HIDDEN_SIZE = 128\n",
    "\n",
    "# We will have 16 episodes per batch\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "# We will only pick top 30% of episodes\n",
    "PERCENTILE = 70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, obs_size, hidden_size, n_actions):\n",
    "        \"\"\"\n",
    "        This model takes a single observation from the environment as an input vector and outputs a number for every action we can perform. \n",
    "        \n",
    "        The output is a probability distribution over actions.\n",
    "        \n",
    "        ARGs:\n",
    "            obs_size    : input dimensions\n",
    "            hidden_size : hidden layer size\n",
    "            n_actions   : output size\n",
    "        \"\"\"\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # Our model\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, n_actions)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HELPER CLASSES\n",
    "\n",
    "# This is a single episode stored as total undiscounted reward and a collection of EpisodeStep\n",
    "Episode = namedtuple('Episode', field_names=['reward', 'steps'])\n",
    "\n",
    "# This will be used to represent one single step that our agent made in the episode, and it stores: observation from the environment and what action the agent completed\n",
    "EpisodeStep = namedtuple('EpisodeStep', field_names=['observation', 'action'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that generates batches\n",
    "def iterate_batches(env, net, batch_size):\n",
    "    \"\"\"\n",
    "    ARGs:\n",
    "        env: (Env class, instance from Gym Library). This is our environment\n",
    "        net: Our neural network\n",
    "        batch_size: count of episodes per batch\n",
    "    \"\"\"\n",
    "    # batch will store a list of Episode instances\n",
    "    batch = []\n",
    "    episode_reward = 0.0\n",
    "    episode_step = []\n",
    "    \n",
    "    # Initializing environment\n",
    "    obs = env.reset()\n",
    "    \n",
    "    # softmax, will convert the network's output to a probability distribution of actions\n",
    "    sm = nn.Softmax(dim=1)\n",
    "    \n",
    "    while True:\n",
    "        \"\"\"\n",
    "        We will now take our observation and:\n",
    "            1. convert to PyTorch Tensor = (1,4) vector\n",
    "            2. run tensor through our NN and through our softmax to give us a distribution \n",
    "            3. Convert tensor into a NumPy array. \n",
    "        \"\"\"\n",
    "        obs_v = torch.Tensor([obs])\n",
    "        act_probs_v = sm(net(obs_v))\n",
    "        act_probs = act_probs_v.data.numpy()[0]\n",
    "        \n",
    "        \"\"\"\n",
    "        We will now use our probability distribution of actions to obtain the actual action for the current step by: sampling this distribution using NumPy's function: random.choice().\n",
    "        \n",
    "        Then we will pass that action we choose to the environment to get our next observation, our reward, and indication whether episode is over or not. \n",
    "        \"\"\"\n",
    "        action = np.random.choice(len(act_probs), p=act_probs)\n",
    "        next_obs, reward, is_done, _ = env.step(action)\n",
    "        \n",
    "        \"\"\"\n",
    "        Now reward will be added to the current episodes total reward, and our list of episode steps is extended with an (observation, action) pair. \n",
    "        \"\"\"\n",
    "        episode_reward += reward\n",
    "        episode_steps.append(EpisodeStep(observation=obs, action=action))\n",
    "        \n",
    "        \"\"\"\n",
    "        The following is for handling when our current episode is over:\n",
    "            1. Append finalized episode to batch, saving the total reward and steps taken\n",
    "            2. Reset our total reward\n",
    "            3. Clean the list of steps\n",
    "            4. Reset out environment & start over\n",
    "        \"\"\"\n",
    "        if is_done:\n",
    "            batch.append(Episode(reward=episode_reward, steps=episode_steps))\n",
    "            episode_reward = 0.0\n",
    "            episode_steps = []\n",
    "            next_obs = env.reset()\n",
    "            \n",
    "            # Checking if batch is full\n",
    "            if len(batch) == batch_size:\n",
    "                yield batch # return\n",
    "                batch = []\n",
    "                \n",
    "        # Assining an observation obtained from the environment to our current observation variable\n",
    "        obs = next_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_batch(batch, percentile):\n",
    "    \"\"\"\n",
    "    This function calculates a boundary reward, which is used to filter elite spides to train on. We used NumPy's percentile function: \n",
    "    \n",
    "        1. From the list of values and the desired percentile\n",
    "        2. Calculates the perentiles value. \n",
    "        3. Then we calculate mean reward (used only for monitoring)\n",
    "    \"\"\"\n",
    "    rewards = list(map(lambda s: s.reward, batch))\n",
    "    reward_bound = np.percentile(rewards, percentile)\n",
    "    reward_mean = float(np.mean(rewards))\n",
    "    \n",
    "    \"\"\"\n",
    "    Now we will filter out our episodes. For every episode in the batch, we will check that the episode has a higher total reward then our boundary and if it has: we will populate lists of observations and actions that we will train on\n",
    "    \"\"\"\n",
    "    train_obs = []\n",
    "    train_act = []\n",
    "    \n",
    "    # Looping through batch\n",
    "    for example in batch:\n",
    "        # does it exceed our boundary?\n",
    "        if example.reward < reward_bound:\n",
    "            continue\n",
    "        \n",
    "        # It does! extend that obersvation\n",
    "        train_obs.extend(map(lambda step: step.observation, example.steps))\n",
    "        \n",
    "        # It does! extend that action \n",
    "        train_act.extend(map(lambda step: step.action, example.steps))\n",
    "        \n",
    "    \"\"\"\n",
    "    Now we will convert out observations and actions from elite episodes into tensors, and return a Tuple of four:\n",
    "        1. observation\n",
    "        2. actions\n",
    "        3. boundary of reward\n",
    "        4. mean of reward\n",
    "    \"\"\"\n",
    "    train_obs_v = torch.Tensor(train_obs)\n",
    "    train_act_v = torch.LongTensor(train_act)\n",
    "    return train_obs_v, train_act_v, reward_bound, reward_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Creating our ennviroment\n",
    "    env = gym.make('CartPole-v0')\n",
    "    # Wrapping with a monitor - so we can observe the agent\n",
    "    env = gym.Wrappers.Monitor(env, directory=\"Monitor\", force=True)\n",
    "    \n",
    "    # Getting our input and output size for our NN\n",
    "    obs_size = env.observation_space.shape[0]\n",
    "    n_actions = env.action_space.n\n",
    "    \n",
    "    # Insantiating our NN\n",
    "    net = Net(obs_size, HIDDEN_SIZE, n_actions)\n",
    "    \n",
    "    # Loss function & Optimizer\n",
    "    objective = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(params=net.parameters(), lr=0.01)\n",
    "    \n",
    "    # for tensorboardX\n",
    "    writer = SummaryWriter()\n",
    "    \n",
    "    # Training\n",
    "    \"\"\"\n",
    "    In this training loop: \n",
    "        1. We will iterate our batches (list of episodes\n",
    "        2. Perfom filtering of the elite episodes using filter_batch function\n",
    "        3. Get our: variables of observations and taken action, reward boundary used for filtering and the mean reward\n",
    "        4. Then we zero gradients of our network\n",
    "        5. Pass observations to the network obtaining its action scores\n",
    "        6. Pass scores to the objective (loss) function, calculating our cross-entropy \n",
    "        \n",
    "    This loop will REINFORCE our network to carry out those 'elite' actions which have led to good rewards. \n",
    "    \n",
    "        7. Calculate gradients on the loss\n",
    "        8. Ask optimizer to adjust network .step()\n",
    "    \"\"\"\n",
    "    for iter_no, batch in enumerate(iterate_batches(env, net, BATCH_SIZE)):\n",
    "        \n",
    "        # This is very similar to training a NN for image classification, sames steps \n",
    "        obs_v, act_v, reward_b, reward_m = filter_batch(batch, PERCENTILE)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        action_scores_v = net(obs_v)\n",
    "        loss_v = objective(action_scores_v, acts_v)\n",
    "        loss_v.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Printing & monitoring our progress\n",
    "        print(f'{iter_no}: loss={loss_v.item()}, reward_mean={reward_m}, reward_bound={reward_b}')\n",
    "        \n",
    "        # for TensorBoard\n",
    "        writer.add_scalar(\"loss\", loss_v_item(), reward_b)\n",
    "        writer.add_scalar(\"reward_bound\", reward_b, iter_no)\n",
    "        writer.add_scalar(\"reward_mean\", reward_m, iter_no)\n",
    "        \n",
    "        \"\"\"\n",
    "        When our mean rewards of our batch episodes becomes greater than 199, we stop our training. \n",
    "        \n",
    "        CartPole environment is considered to be solved when the mean reward for last 100 episodes is greater than 195\n",
    "        \"\"\"\n",
    "        if reward_m > 199:\n",
    "            print('Solved!')\n",
    "            break\n",
    "            \n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting It All Together\n",
    "So if you still have confusion, here is what is essentially happening when you run ```cart_pole_pytorch.py```\n",
    "\n",
    "After initializing our: Neural Network, Optimizer, Loss-Function, and Environment we enter into **inference** training. \n",
    "\n",
    "That is, we train our Neural Network at inference. As the Agent navigates and plays the game it is learning. *Close to how actual AGI will work*. \n",
    "\n",
    "In the very beginning our Agent's NN is initialized with random weights. Therefor it will perform badly, and will end an episodes very quickly. \n",
    "\n",
    "The first function that is called (called on loop) is ```iterate_batches()``` which will return a batch. Since we use ```enumerate``` it will return a batch & iteration number to keep track. \n",
    "\n",
    "In our example a batch is composed of **16 episodes**. Remember an episode is essentially an entire game played. \n",
    "\n",
    "We then take this batch (we process one batch at a time) and run it through ```filter_batch()``` which in our case will return **top 30%** episodes (observations and associated actions). \n",
    "\n",
    "We will then use what ```filter_batch()``` returns: ```obs_v, acts_v, reward_b, reward_m```. Most important we will use: ```obs_v``` as our input to our neural network. ```acts_v``` as the **true labels** to the input. \n",
    "\n",
    "So now that we have these variables, we will first run ```obs_v``` into our (at first = untrained neural network) to give us a prediction (softmax) of actions to take. \n",
    "\n",
    "Running ```obs_v``` into our neural network will return ```action_scores``` which is predicted actions to take. \n",
    "\n",
    "We will then use ```nn.CrossEntropyLoss(action_scores, acts_v)``` to compare: **predicted**: ```action_scores``` to **actual**: ```acts_v``` to calculate our loss. \n",
    "\n",
    "With our loss calculated we will do (as you usually do for NN training):\n",
    "* calculate gradients: ```loss_v.backward()```\n",
    "* call optimizer step: ```optimizer.step()```\n",
    "\n",
    "Then we repeat! \n",
    "\n",
    "What happens is: every iteration is an iteration of **16 batches**. As the loss goes down, the agent last longer and keeps track of the obervations and actions. \n",
    "\n",
    "With gradient descent it will eventually learn the steps to take in order to achieve the goal! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
